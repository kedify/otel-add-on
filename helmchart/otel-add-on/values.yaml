#otel-add-on:
#------------
replicaCount: 1
namespace: keda
## -- when rendering/installing/upgrading the helm chart, the schema validation trivially passes
#disableSchemaValidation: false

image:
  # -- Image to use for the Deployment
  repository: ghcr.io/kedify/otel-add-on
  # -- Image pull policy, consult [docs](https://kubernetes.io/docs/concepts/containers/images/#image-pull-policy)
  pullPolicy: Always
  # -- Image version to use for the Deployment, if not specified, it defaults to `.Chart.AppVersion`
  tag: ""

settings:
  metricStore:
    # -- how long the metrics should be kept in the short term (in memory) storage
    retentionSeconds: 120

    # -- if enabled, no metrics will be stored until there is a request for such metric
    # from KEDA operator.
    lazySeries: false

    # -- if enabled, the only aggregate that will be calculated on the fly is the one referenced in the metric query
    #  (by default, we calculate and store all of them - sum, rate, min, max, etc.)
    lazyAggregates: false

    # -- when enabled, the scaler will be returning error to KEDA's `GetMetrics()` call
    errIfNotFound: false

    # -- default value, that is reported in case of error or if the value is not in the mem store
    valueIfNotFound: 0.

  # -- how often (in milliseconds) should the IsActive method be tried
  isActivePollingIntervalMilliseconds: 500

  # -- internal (mostly golang) metrics will be exposed on `:8080/metrics`
  internalMetricsPort: 8080

  # -- port where rest api should be listening
  restApiPort: 9090

  logs:
    # -- Can be one of 'debug', 'info', 'error', or any integer value > 0
    # which corresponds to custom debug levels of increasing verbosity
    logLvl: info

    # -- one of: info, error, panic
    stackTracesLvl: error

    # -- if anything else than 'false', the log will not contain colors
    noColor: false

    # -- if anything else than 'false', the log will not print the ascii logo
    noBanner: false

# -- one of: RollingUpdate, Recreate ([details](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy))
deploymentStrategy: RollingUpdate

# -- when disabled, the deployment with KEDA Scaler will not be rendered
deployScaler: true

validatingAdmissionPolicy:
  # -- whether the ValidatingAdmissionPolicy and ValidatingAdmissionPolicyBinding resources should be also rendered
  enabled: false
  name: well-formed-otel-scalers

# -- should the ascii logo be printed when this helm chart is installed
asciiArt: true

# -- [details](https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod)
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # -- should the service account be also created and linked in the deployment
  create: true
  automount: true
  # -- further custom annotation that will be added on the service account
  annotations: {}
  # -- name of the service account, defaults to `otel-add-on.fullname` ~ release name if not overriden
  name: ""

# -- additional custom pod annotations that will be used for pod
podAnnotations: {}

# -- additional custom pod labels that will be used for pod
podLabels: {}

# -- [details](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-the-security-context-for-a-pod)
podSecurityContext: {}

securityContext:
  capabilities:
    drop:
      - ALL
  # -- [details](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/)
  readOnlyRootFilesystem: true
  # -- [details](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#implicit-group-memberships-defined-in-etc-group-in-the-container-image)
  runAsNonRoot: true
  # -- [details](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#implicit-group-memberships-defined-in-etc-group-in-the-container-image)
  runAsUser: 1000

service:
  # -- Under this service, the otel add on needs to be reachable by KEDA operator and OTel collector
  # ([details](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types))
  type: ClusterIP
  # -- OTLP receiver will be opened on this port. OTel exporter configured in the OTel collector needs to have this value set.
  otlpReceiverPort: 4317
  # -- KEDA external scaler will be opened on this port. ScaledObject's `.spec.triggers[].metadata.scalerAddress` needs to be set to this svc and this port.
  kedaExternalScalerPort: 4318

resources:
  limits:
    # -- cpu limit for the pod, enforced by cgroups ([details](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/))
    cpu: 500m
    # -- memory limit for the pod, used by oomkiller ([details](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/))
    memory: 256Mi
  requests:
    # -- cpu request for the pod, used by k8s scheduler ([details](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/))
    cpu: 500m
    # -- memory request for the pod, used by k8s scheduler ([details](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/))
    memory: 128Mi

volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

# -- [details](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector)
nodeSelector: {}

# -- [details](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/)
tolerations: []

# -- [details](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity)
affinity: {}

# -- helper container image that creates the OpenTelemetryCollector CR as post-install helm hook
kubectlImage:
  tag: "v1.33.1"
  repository: ghcr.io/kedify/kubectl
  pullPolicy: Always
  pullSecrets: []

# -- default template for OpenTelemetryCollector CR. Override the specifics under `otelOperatorCrs` section
otelOperatorCrDefaultTemplate:
  # -- container image for post-install helm hook that help with OpenTelemetryCollector CR installation
  debug: false

  # -- how the otel collector should be deployed: sidecar, statefulset, deployment, daemonset
  mode: sidecar
  # -- whether TargetAllocator feature (Prometheus Custom Resources for service discovery) should be enabled ([details](https://github.com/open-telemetry/opentelemetry-operator?tab=readme-ov-file#target-allocator))
  # make sure the mode is not set to `sidecar` when this is enabled
  targetAllocatorEnabled: false
  # -- list of existing cluster roles that will be bound to the service account (in order to be able to work with `{Pod,Service}Monitor` CRD)
  targetAllocatorClusterRoles:
    - kube-prometheus-stack-operator
    - kube-prometheus-stack-prometheus
  targetAllocator:
    serviceAccount: otel-prom-reader
    prometheusCR:
      enabled: true
      # -- further narrow the ServiceMonitor CRs
      serviceMonitorSelector: {}
      #        matchLabels:
      #          foo: bar
      # -- further narrow the ServiceMonitor CRs
      podMonitorSelector: {}
  serviceAccount:
    create: false
    annotations: {}
    name: ""
  clusterRole:
    create: false
    annotations: {}
    rules: []
  #    - apiGroups:
  #      - ''
  #      resources:
  #      - 'pods'
  #      - 'nodes'
  #      verbs:
  #      - 'get'
  #      - 'list'
  #      - 'watch'
  # -- resources for the OTel collector container
  resources:
    limits:
      cpu: 400m
      memory: 128Mi
    requests:
      cpu: 200m
      memory: 64Mi
  # -- free form OTel configuration that will be used for the OpenTelemetryCollector CR (no checks)
  # this is mutually exclusive w/ all the following options
  alternateOtelConfig: {}

  # -- static targets for prometheus receiver, this needs to take into account the deployment mode of the collector
  # (127.0.0.1 in case of a sidecar mode will mean something else than for statefulset mode)
  prometheusScrapeConfigs:
    - job_name: 'otel-collector'
      scrape_interval: 3s
      static_configs:
        - targets: [ '0.0.0.0:8080' ]
  # -- mutually exclusive with prometheusScrapeConfigs option
  alternateReceivers: {}
  #    k8s_events:
  #      namespaces: [ kube-system ]
  # -- if not empty, only following metrics will be sent. This translates to filter/metrics processor. Empty array means include all.
  includeMetrics: []
  #    - vllm:gpu_cache_usage_perc
  #    - vllm:gpu_cache_usage_perc_scaled
  #    - customload
  metricsgeneration:
    rules: []
  #      - name: vllm:gpu_cache_usage_perc_scaled
  #        type: scale
  #        metric1: vllm:gpu_cache_usage_perc
  #        operation: multiply
  #        scale_by: 100
  #      - name: customload
  #        type: calculate
  #        metric1: vllm:gpu_cache_usage_perc
  #        metric2: vllm:num_requests_waiting
  #        operation: multiply
  alternateExtensions: {}
  #    bearertokenauth/github:
  #      token: ${env:GH_PAT}

  envFrom: []
  #  - secretRef:
  #      name: gh-token
  env: []
#    - name: GH_PAT
#      value: "**"

# -- create also OpenTelemetryCollector CRs that will be reconciled by OTel Operator
# it takes all the default settings defined in `otelOperatorCrDefaultTemplate` and allows overriding them
otelOperatorCrs:
  # -- if enabled, the OpenTelemetryCollector CR will be created using post-install hook job_name
- enabled: false
  # -- name of the OpenTelemetryCollector CR. If left empty, the release name will be used.
  name: ""
  # -- in what k8s namespace the OpenTelemetryCollector CR should be created. If left empty, the release namespace will be used.
  namespace: ""

- name: target-allocator
  enabled: false
  targetAllocatorEnabled: true
  mode: deployment


# otel operator helm chart:
# https://github.com/open-telemetry/opentelemetry-helm-charts/blob/opentelemetry-operator-0.88.6/charts/opentelemetry-operator/values.yaml
otelOperator:
  enabled: false
  fullnameOverride: otel-operator
  manager:
    collectorImage:
#      repository: otel/opentelemetry-collector-k8s
      repository: otel/opentelemetry-collector-contrib
    env: {}
    extraEnvs:
    - name: ENABLE_WEBHOOKS
      value: "false"
    serviceAccount:
      name: otel-operator

  admissionWebhooks:
    create: false
# otel collector helm chart:
# https://github.com/open-telemetry/opentelemetry-helm-charts/tree/opentelemetry-collector-0.110.0/charts/opentelemetry-collector/values.yaml
#--------------------------
# example configuration of otel collector to work as an opencensus receiver and forwarder to otel-add-on
otelCollector:
  enabled: false
  # -- Valid values are "daemonset", "deployment", "sidecar" and "statefulset"
  mode: deployment
  image:
    repository: otel/opentelemetry-collector-k8s
  fullnameOverride: otelcol

  ports:
    opencensus:
      enabled: true
      containerPort: 55678
      servicePort: 55678
      hostPort: 55678
      protocol: TCP
  alternateConfig:
    receivers:
      # https://grafana.com/docs/alloy/latest/reference/components/otelcol/otelcol.receiver.opencensus/
      opencensus:
        endpoint: 0.0.0.0:55678
        include_metadata: true

    processors:
      resourcedetection/env:
        detectors: [ env ]
        timeout: 2s
        override: false
      transform:
        metric_statements:
          - context: datapoint
            statements:
              - set(attributes["namespace"], resource.attributes["k8s.namespace.name"])
              - set(attributes["pod"], resource.attributes["k8s.pod.name"])
              - set(attributes["deployment"], resource.attributes["k8s.deployment.name"])
    exporters:
      otlp:
        # make sure this is the same hostname and port as .service (when using different namespace)
        endpoint: keda-otel-scaler.keda.svc:4317
        compression: "none"
        tls:
          insecure: true
      debug:
        verbosity: detailed

    service:
      extensions:
        - health_check
      pipelines:
        metrics:
          receivers: [opencensus]
          processors: [resourcedetection/env, transform]
          exporters: [debug, otlp]

    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
