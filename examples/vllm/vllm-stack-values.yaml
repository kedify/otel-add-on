servingEngineSpec:
  runtimeClassName: ""
  modelSpec:
    - name: "llama3"
      repository: "vllm/vllm-openai"
      # tag: "latest"
      tag: "v0.9.1"
      modelURL: "meta-llama/Llama-3.1-8B-Instruct"
      replicaCount: 1

      requestCPU: 10
      requestMemory: "16Gi"
      requestGPU: 1
      pvcStorage: "50Gi"
      pvcAccessMode:
        - ReadWriteOnce
      podAnnotations:
        sidecar.opentelemetry.io/inject: "otel-sidecar-template"

      vllmConfig:
        enableChunkedPrefill: false
        enablePrefixCaching: false
        maxModelLen: 4096
        dtype: "bfloat16"
        extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.8"]

      # pass this as --set arg so that it doesn't end up in git
      # hf_token: hf_**
