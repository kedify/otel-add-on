#otel-add-on:
#------------
replicaCount: 1
namespace: ""
## -- when rendering/installing/upgrading the helm chart, the schema validation trivially passes
#disableSchemaValidation: false

image:
  # -- Image to use for the Deployment
  repository: ghcr.io/kedify/otel-add-on
  # -- (k8s/containers/images/#image-pull-policy) image pull policy for KEDA OTel Scaler pod
  pullPolicy: Always
  # -- Image version to use for the Deployment, if not specified, it defaults to `.Chart.AppVersion`
  tag: ""

settings:
  metricStore:
    # -- how long the metrics should be kept in the short term (in memory) storage
    retentionSeconds: 120

    # -- if enabled, no metrics will be stored until there is a request for such metric
    # from KEDA operator.
    lazySeries: false

    # -- if enabled, the only aggregate that will be calculated on the fly is the one referenced in the metric query
    #  (by default, we calculate and store all of them - sum, rate, min, max, etc.)
    lazyAggregates: false

    # -- when enabled, the scaler will be returning error to KEDA's `GetMetrics()` call
    errIfNotFound: false

    # -- default value, that is reported in case of error or if the value is not in the mem store
    valueIfNotFound: 0.

  # -- how often (in milliseconds) should the IsActive method be tried
  isActivePollingIntervalMilliseconds: 500

  # -- internal (mostly golang) metrics will be exposed on `:8080/metrics`
  internalMetricsPort: 8080

  # -- port where rest api should be listening
  restApiPort: 9090

  logs:
    # -- Can be one of 'debug', 'info', 'error', or any integer value > 0
    # which corresponds to custom debug levels of increasing verbosity
    logLvl: info

    # -- one of: info, error, panic
    stackTracesLvl: error

    # -- if anything else than 'false', the log will not contain colors
    noColor: false

    # -- if anything else than 'false', the log will not print the ascii logo
    noBanner: false
  
  tls:
    # -- (optional) path to CA certificate. When provided, the client certificate will be verified using this CA where "client" ~ another OTLP exporter.
    caFile: ""
    # -- (optional) path to TLS certificate that will be used for OTLP receiver
    certFile: ""
    # -- (optional) path to TLS key that will be used for OTLP receiver
    keyFile: ""
    # -- (optional) specifies the duration after which the certificates will be reloaded.
    # This is useful when using the CertManager for rotating the certs mounted as Secrets.
    reloadInterval: "5m"
    # -- (optional) list of secrets that will be mounted to deployment's pod. One entry in this list, will create one volume and one volumeMount for pod.
    # This is a convenient way for mounting the certs for TLS, but using `.volumes & .volumeMounts` for anything advanced will also work.
    secrets: []
#      - name: root-ca
#        mountPath: /etc/ca-certificate
#      - name: server-tls
#        mountPath: /etc/server-tls

# -- (k8s/workloads/controllers/deployment/#strategy) one of: RollingUpdate, Recreate
deploymentStrategy: RollingUpdate

# -- when disabled, the deployment with KEDA Scaler will not be rendered
deployScaler: true

validatingAdmissionPolicy:
  # -- whether the ValidatingAdmissionPolicy and ValidatingAdmissionPolicyBinding resources should be also rendered
  enabled: false
  name: well-formed-otel-scalers

# -- should the ascii logo be printed when this helm chart is installed
asciiArt: true

# -- (k8s/containers/images/#specifying-imagepullsecrets-on-a-pod) imagePullSecrets for KEDA OTel Scaler pod
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # -- should the service account be also created and linked in the deployment
  create: true
  automount: true
  # -- further custom annotation that will be added on the service account
  annotations: {}
  # -- name of the service account, defaults to `otel-add-on.fullname` ~ release name if not overriden
  name: ""

# -- additional custom pod annotations that will be used for pod
podAnnotations: {}

# -- additional custom pod labels that will be used for pod
podLabels: {}

# -- (k8s/security/pod-security-standards) securityContext for KEDA OTel Scaler pod
podSecurityContext: {}

# -- (k8s/security/pod-security-standards) securityContext for KEDA OTel Scaler container
# @notationType -- yaml
securityContext:
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000

service:
  # -- (k8s/services-networking/service/#publishing-services-service-types) Under this service, the otel add on needs to be reachable by KEDA operator and OTel collector
  type: ClusterIP
  # -- OTLP receiver will be opened on this port. OTel exporter configured in the OTel collector needs to have this value set.
  otlpReceiverPort: 4317
  # -- KEDA external scaler will be opened on this port. ScaledObject's `.spec.triggers[].metadata.scalerAddress` needs to be set to this svc and this port.
  kedaExternalScalerPort: 4318

# -- (k8s/configuration/manage-resources-containers) resources for the OTel Scaler pod
# @notationType -- yaml
resources:
  limits:
    cpu: 500m
    memory: 256Mi
  requests:
    cpu: 500m
    memory: 128Mi

volumes: []
# - name: foo
#   secret:
#     secretName: mysecret
#     optional: false

volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

# -- (k8s/scheduling-eviction/assign-pod-node/#nodeselector) node selector for KEDA OTel Scaler pod
nodeSelector: {}

# -- (k8s/scheduling-eviction/taint-and-toleration) tolerations for KEDA OTel Scaler pod
tolerations: []

# -- (k8s/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity) affinity for KEDA OTel Scaler pod
affinity: {}

# -- helper container image that creates the OpenTelemetryCollector CR as post-install helm hook
# @notationType -- yaml
kubectlImage:
  tag: "v1.33.1"
  repository: ghcr.io/kedify/kubectl
  pullPolicy: Always
  pullSecrets: []

# -- (raw)
# @raw
# @notationType -- yaml
# **This field defines the default template for `OpenTelemetryCollector` CR**
#
# Vast majority of the fields has its counterpart described in OpenTelemetryCollector CRD.
# In order to check their description, install the CRD and run:
# ```bash
#  kubectl explain otelcol.spec
# ```
# These defaults are then used as a base layer of configuration for all the items in the `.otelOperatorCrs` list.
# So given we have this values:
#
# ```yaml
# otelOperatorCrDefaultTemplate:
#   mode: deployment
# otelOperatorCrs:
#   - enabled: true
#     name: "foo"
#     mode: "daemonset"
#   - enabled: true
#     name: "bar"
# ```
# It will render[^fn1] two OpenTelemetryCollector CRs called `foo` and `bar` where `foo` will have the `.spec.mode` set to
# `daemonset` and `foo` will inherit the default mode from `.otelOperatorCrDefaultTemplate.mode` => `deployment`
# [^fn1]: Well in fact it doesn't render the OpenTelemetryCollector CRs directly, but nested as part of a ConfigMap. Then this
# CM is read durin post-install hook and CR is created. This is because we can't render CRD and its instances in one helm command.
#
#  > [!NOTE]
#  > When specifying custom receivers, processors, exporters or extensions. Use `alternate{Receivers,Processors,Exporters,Extensions}`.
#  > And there is no need to enable these under pipeline section. This is done automagically [here](https://github.com/kedify/otel-add-on/blob/main/helmchart/otel-add-on/templates/install-otc/otc-configmap.yaml).
#
#  > [!TIP]
#  > For overriding the whole OTel config, use the `.alternateOtelConfig` field.
#
# Advanced example:
# <details>
# <summary>Expand</summary>
#
# `values.yaml:`
# ```yaml
# otelOperator:
#   enabled: true
# otelOperatorCrDefaultTemplate:
#   mode: deployment
#   alternateReceivers:
#     otlp:
#       protocols:
#         grpc:
#           endpoint: 0.0.0.0:4317
#   alternateExporters:
#     otlp:
#       endpoint: col-fanout-collector:4317
#       tls:
#         insecure: true
# otelOperatorCrs:
#   - enabled: true
#     name: "col-in-1"
#   - enabled: true
#     name: "col-in-2"
#   - enabled: true
#     name: "col-fanout"
#     alternateExporters:
#       otlp/external:
#         endpoint: external-collector:4317
#       otlp/keda:
#         endpoint: keda-otel-scaler.keda.svc:4317
#         tls:
#           insecure: true
#     alternateConnectors:
#       routing:
#         default_pipelines: [metrics/all]
#         table:
#           - context: metric
#             condition: metric.name == "http_requests_total"
#             pipelines: [metrics/keda]
#     alternatePipelines:
#       metrics/in:
#         receivers: [otlp]
#         exporters: [routing]
#       metrics/keda:
#         receivers: [routing]
#         exporters: [otlp/keda]
#       metrics/all:
#         receivers: [routing]
#         exporters: [otlp/external]
# ```
#  resulting architecture:
# ```mermaid
# graph TD;
#     A[col-in-1] -- metrics --> C{col-fanout}
#     B[col-in-2] -- metrics, traces --> C{col-fanout}
#     C -- one metric --> D(KEDA Scaler)
#     C -- all --> E((external-col))
# ```
#
# </details>
# @notationType -- tpl/yaml
otelOperatorCrDefaultTemplate:
  # -- container image for post-install helm hook that help with OpenTelemetryCollector CR installation
  debug: false

  # -- how the otel collector should be deployed: sidecar, statefulset, deployment, daemonset
  # note: make sure the CertManager is installed and admission webhooks are enabled for the OTel operator when using mode=sidecar
  mode: deployment
  # -- whether TargetAllocator feature (Prometheus Custom Resources for service discovery) should be enabled ([details](https://github.com/open-telemetry/opentelemetry-operator?tab=readme-ov-file#target-allocator))
  # make sure the mode is not set to `sidecar` when this is enabled
  targetAllocatorEnabled: false
  # -- list of existing cluster roles that will be bound to the service account (in order to be able to work with `{Pod,Service}Monitor` CRD)
  targetAllocatorClusterRoles:
    - kube-prometheus-stack-operator
    - kube-prometheus-stack-prometheus
  targetAllocator:
    serviceAccount: otel-prom-reader
    enabled: true
    prometheusCR:
      enabled: true
      # -- further narrow the ServiceMonitor CRs (labels)
      serviceMonitorSelector: {}
      #        matchLabels:
      #          foo: bar
      # -- further narrow the PodMonitor CRs
      podMonitorSelector: {}
      allowNamespaces: []
      denyNamespaces: []
  serviceAccount:
    create: false
    annotations: {}
    name: ""
  # -- TLS settings for OTel collector's exporter that feeds the metrics to KEDA OTel scaler
  # it is not in scope of this helm chart to create the secrets with certificate, however this is a convenient way
  # of configuring volumes and volumeMounts for each secret with cert. It has the same structure as tls settings for
  # the scaler (check .Values.tls). One significant difference is that here we specify a client cert for OTLP exporter, while
  # .Values.tls specify the server cert for OTLP receiver
  tls: {}
#    caFile: "/etc/ca-certificate/rootCA.crt"
#    certFile: "/etc/client-tls/tls.crt"
#    keyFile: "/etc/client-tls/tls.key"
#    # https://github.com/open-telemetry/opentelemetry-collector/blob/main/config/configtls/README.md
#    extraSettings:
#      reload_interval: 15s
#    secrets:
#      - name: root-ca
#        mountPath: /etc/ca-certificate
#      - name: client-tls
#        mountPath: /etc/client-tls

  clusterRole:
    create: false
    annotations: {}
    rules: []
  #    - apiGroups:
  #      - ''
  #      resources:
  #      - 'pods'
  #      - 'nodes'
  #      verbs:
  #      - 'get'
  #      - 'list'
  #      - 'watch'
  # -- (k8s/configuration/manage-resources-containers) resources for the OTel collector container
  # @notationType -- yaml
  resources:
    limits:
      cpu: 400m
      memory: 128Mi
    requests:
      cpu: 200m
      memory: 64Mi
  # -- free form OTel configuration that will be used for the OpenTelemetryCollector CR (no checks)
  # this is mutually exclusive w/ all the following options
  alternateOtelConfig: {}

  # -- static targets for prometheus receiver, this needs to take into account the deployment mode of the collector
  # (127.0.0.1 in case of a sidecar mode will mean something else than for statefulset mode)
  prometheusScrapeConfigs:
    - job_name: 'otel-collector'
      scrape_interval: 3s
      static_configs:
        - targets: [ '0.0.0.0:8080' ]
  # -- mutually exclusive with prometheusScrapeConfigs option
  alternateReceivers: {}
  #    k8s_events:
  #      namespaces: [ kube-system ]
  # -- if not empty, only following metrics will be sent. This translates to filter/metrics processor. Empty array means include all.
  includeMetrics: []
  #    - vllm:gpu_cache_usage_perc
  #    - vllm:gpu_cache_usage_perc_scaled
  #    - customload
  alternateExporters: {}
#    debug:
#      verbosity: detailed
  metricsgeneration:
    rules: []
  #      - name: vllm:gpu_cache_usage_perc_scaled
  #        type: scale
  #        metric1: vllm:gpu_cache_usage_perc
  #        operation: multiply
  #        scale_by: 100
  #      - name: customload
  #        type: calculate
  #        metric1: vllm:gpu_cache_usage_perc
  #        metric2: vllm:num_requests_waiting
  #        operation: multiply
  alternateExtensions: {}
  #    bearertokenauth/github:
  #      token: ${env:GH_PAT}
  alternateConnectors: {}
  alternatePipelines: {}

  envFrom: []
  #  - secretRef:
  #      name: gh-token
  env: []
#    - name: GH_PAT
#      value: "**"

# -- create also OpenTelemetryCollector CRs that will be reconciled by OTel Operator
# it takes all the default settings defined in `otelOperatorCrDefaultTemplate` and allows overriding them here
# @notationType -- yaml
otelOperatorCrs:
  # -- if enabled, the OpenTelemetryCollector CR will be created using post-install hook job_name
- enabled: false
  # -- name of the OpenTelemetryCollector CR. If left empty, the release name will be used.
  name: ""
  # -- in what k8s namespace the OpenTelemetryCollector CR should be created. If left empty, the release namespace will be used.
  namespace: ""

- name: target-allocator
  enabled: false
  targetAllocatorEnabled: true
  mode: deployment


# -- values for OTel operator helm chart - these values overrides the defaults defined [here](https://github.com/open-telemetry/opentelemetry-helm-charts/blob/opentelemetry-operator-0.92.3/charts/opentelemetry-operator/values.yaml)
# by default the operator is `disabled`
# @notationType -- yaml
otelOperator:
  enabled: false
  fullnameOverride: otel-operator
  manager:
    collectorImage:
#      repository: otel/opentelemetry-collector-k8s
      repository: otel/opentelemetry-collector-contrib
    env:
      ENABLE_WEBHOOKS: "false"
    serviceAccount:
      name: otel-operator

  admissionWebhooks:
    create: false

# -- values for OTel collector helm chart - these values overrides the defaults defined [here](https://github.com/open-telemetry/opentelemetry-helm-charts/tree/opentelemetry-collector-0.110.0/charts/opentelemetry-collector/values.yaml)
# by default the collector is `disabled`
# @notationType -- yaml
otelCollector:
  # -- If enabled, the OTel collector sub-chart will be rendered
  enabled: false
  # -- Valid values are "daemonset", "deployment", "sidecar" and "statefulset"
  mode: deployment
  image:
    # -- Container image - OTel collector distribution
    repository: otel/opentelemetry-collector-k8s
  fullnameOverride: otelcol
  ports:
    opencensus:
      enabled: true
      containerPort: 55678
      servicePort: 55678
      hostPort: 55678
      protocol: TCP
  # -- Configuration for OTel collector that will be installed
  # @notationType -- yaml
  alternateConfig:
    receivers:
      # https://grafana.com/docs/alloy/latest/reference/components/otelcol/otelcol.receiver.opencensus/
      opencensus:
        endpoint: 0.0.0.0:55678
        include_metadata: true

    processors:
      resourcedetection/env:
        detectors: [ env ]
        timeout: 2s
        override: false
      transform:
        metric_statements:
          - context: datapoint
            statements:
              - set(attributes["namespace"], resource.attributes["k8s.namespace.name"])
              - set(attributes["pod"], resource.attributes["k8s.pod.name"])
              - set(attributes["deployment"], resource.attributes["k8s.deployment.name"])
    exporters:
      otlp:
        # make sure this is the same hostname and port as .service (when using different namespace)
        endpoint: keda-otel-scaler.keda.svc:4317
        compression: "none"
        tls:
          insecure: true
      debug:
        verbosity: detailed

    service:
      extensions:
        - health_check
      pipelines:
        metrics:
          receivers: [opencensus]
          processors: [resourcedetection/env, transform]
          exporters: [debug, otlp]

    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133
